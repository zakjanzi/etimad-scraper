import asyncio
import json
import os
import csv
from typing import List

from crawl4ai import AsyncWebCrawler, BrowserConfig, CacheMode, CrawlerRunConfig, LLMConfig
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from pydantic import BaseModel, Field

# Base URL for scraping (page number will be appended)
BASE_URL = "https://tenders.etimad.sa/Tender/AllTendersForVisitor?PageNumber={page}"

# Instruction to the LLM to extract the required fields
INSTRUCTION_TO_LLM = """
Extract the following fields from each tender opportunity. Some fields are visible on the card, 
but most require opening each tender to view details:
1. Title of the project ("ÿßÿ≥ŸÖ ÿßŸÑŸÖŸÜÿßŸÅÿ≥ÿ©")
2. Description ("ÿßŸÑÿ∫ÿ±ÿ∂ ŸÖŸÜ ÿßŸÑŸÖŸÜÿßŸÅÿ≥ÿ©")
3. Reference Number ("ÿßŸÑÿ±ŸÇŸÖ ÿßŸÑŸÖÿ±ÿ¨ÿπŸä")
4. Contract Duration ("ŸÖÿØÿ© ÿßŸÑÿπŸÇÿØ")
5. Governmental Entity ("ÿßŸÑÿ¨Ÿáÿ© ÿßŸÑÿ≠ŸÉŸàŸÖŸäÿ©")
6. Questions Deadline ("ÿ¢ÿÆÿ± ŸÖŸàÿπÿØ ŸÑÿ•ÿ≥ÿ™ŸÑÿßŸÖ ÿßŸÑÿ•ÿ≥ÿ™ŸÅÿ≥ÿßÿ±ÿßÿ™")
7. Bid Deadline ("ÿ¢ÿÆÿ± ŸÖŸàÿπÿØ ŸÑÿ™ŸÇÿØŸäŸÖ ÿßŸÑÿπÿ±Ÿàÿ∂")
8. Location ("ŸÖŸÉÿßŸÜ ÿßŸÑÿ™ŸÜŸÅŸäÿ∞")
9. Date of Publication ("ÿ™ÿßÿ±ŸäÿÆ ÿßŸÑŸÜÿ¥ÿ±")
10. Categories/Tags ("ŸÜÿ¥ÿßÿ∑ ÿßŸÑŸÖŸÜÿßŸÅÿ≥ÿ©")
11. Bid Price ("ŸÇŸäŸÖÿ© Ÿàÿ´ÿßÿ¶ŸÇ ÿßŸÑŸÖŸÜÿßŸÅÿ≥ÿ©")
"""

# Define a Pydantic model for the extracted data
class TenderOpportunity(BaseModel):
    title: str = Field(..., alias="ÿßÿ≥ŸÖ ÿßŸÑŸÖŸÜÿßŸÅÿ≥ÿ©")
    description: str = Field(..., alias="ÿßŸÑÿ∫ÿ±ÿ∂ ŸÖŸÜ ÿßŸÑŸÖŸÜÿßŸÅÿ≥ÿ©")
    reference_number: str = Field(..., alias='ÿßŸÑÿ±ŸÇŸÖ ÿßŸÑŸÖÿ±ÿ¨ÿπŸä')
    contract_duration: str = Field(..., alias="ŸÖÿØÿ© ÿßŸÑÿπŸÇÿØ")
    governmental_entity: str = Field(..., alias="ÿßŸÑÿ¨Ÿáÿ© ÿßŸÑÿ≠ŸÉŸàŸÖŸäÿ©")
    questions_deadline: str = Field(..., alias="ÿ¢ÿÆÿ± ŸÖŸàÿπÿØ ŸÑÿ•ÿ≥ÿ™ŸÑÿßŸÖ ÿßŸÑÿ•ÿ≥ÿ™ŸÅÿ≥ÿßÿ±ÿßÿ™")
    bid_deadline: str = Field(..., alias="ÿ¢ÿÆÿ± ŸÖŸàÿπÿØ ŸÑÿ™ŸÇÿØŸäŸÖ ÿßŸÑÿπÿ±Ÿàÿ∂")
    location: str = Field(..., alias="ŸÖŸÉÿßŸÜ ÿßŸÑÿ™ŸÜŸÅŸäÿ∞")
    date_of_publication: str = Field(..., alias="ÿ™ÿßÿ±ŸäÿÆ ÿßŸÑŸÜÿ¥ÿ±")
    categories_tags: str = Field(..., alias="ŸÜÿ¥ÿßÿ∑ ÿßŸÑŸÖŸÜÿßŸÅÿ≥ÿ©")
    bid_price: str = Field(..., alias="ŸÇŸäŸÖÿ© Ÿàÿ´ÿßÿ¶ŸÇ ÿßŸÑŸÖŸÜÿßŸÅÿ≥ÿ©")

async def scrape_page(crawler: AsyncWebCrawler, crawl_config: CrawlerRunConfig, page: int):
    """Scrape a single page of tenders"""
    url = BASE_URL.format(page=page)
    print(f"üîÑ Scraping page {page}...")
    result = await crawler.arun(url=url, config=crawl_config)
    
    if result.success:
        data = json.loads(result.extracted_content)
        print(f"‚úÖ Found {len(data)} tenders on page {page}")
        return data
    else:
        print(f"‚ùå Error scraping page {page}: {result.error_message}")
        return []

async def main():
    # Configure LLM
    llm_config = LLMConfig(
        provider="deepseek/deepseek-chat",
        api_token=os.getenv("DEEPSEEK_API"),
    )

    # Configure LLM extraction strategy with higher limits
    llm_strategy = LLMExtractionStrategy(
        llm_config=llm_config,
        schema=TenderOpportunity.model_json_schema(),
        extraction_type="schema",
        instruction=INSTRUCTION_TO_LLM,
        chunk_token_threshold=2000,  # Increased from 1000
        overlap_rate=0.1,  # Small overlap helps with continuity
        apply_chunking=True,
        input_format="markdown",
        extra_args={
            "temperature": 0.0,
            "max_tokens": 2000,  # Increased from 800
        },
    )

    # Configure the crawler with valid parameters
    crawl_config = CrawlerRunConfig(
        extraction_strategy=llm_strategy,
        cache_mode=CacheMode.BYPASS,
        process_iframes=True,  # Changed to True for better JS support
        remove_overlay_elements=True,
        exclude_external_links=True,
        word_count_threshold=50,  # Lower threshold to catch more items
        #wait_for="networkidle",
        js_code=[
            "window.scrollTo(0, document.body.scrollHeight);",  # Scroll to bottom
            "new Promise(resolve => setTimeout(resolve, 2000))"  # Wait 2 seconds
        ],
        verbose=True,
    )

    # Configure the browser
    browser_cfg = BrowserConfig(
        headless=True,
        verbose=True,
        viewport_width=1280,  # Larger viewport helps with responsive sites
        viewport_height=2000,  # Taller viewport to show more items
    )

    all_tenders = []
    
    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        # Scrape multiple pages (adjust range as needed)
        for page in range(1, 6):  # Pages 1 through 5
            page_data = await scrape_page(crawler, crawl_config, page)
            all_tenders.extend(page_data)
            
            # Small delay between pages to avoid rate limiting
            await asyncio.sleep(2)

    # Save all collected data to CSV
    if all_tenders:
        csv_file = "tender_opportunities.csv"
        with open(csv_file, mode='w', encoding='utf-8', newline='') as file:
            writer = csv.DictWriter(file, fieldnames=all_tenders[0].keys())
            writer.writeheader()
            writer.writerows(all_tenders)
        
        print(f"\nüéâ Success! Saved {len(all_tenders)} tenders to {csv_file}")
    else:
        print("‚ùå No tender data was collected")

if __name__ == "__main__":
    asyncio.run(main())