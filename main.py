import asyncio
import json
import os
import csv
from typing import List

from crawl4ai import AsyncWebCrawler, BrowserConfig, CacheMode, CrawlerRunConfig, LLMConfig
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from pydantic import BaseModel, Field

# Base URL for scraping (page number will be appended)
BASE_URL = "https://tenders.etimad.sa/Tender/AllTendersForVisitor?PageNumber={page}"

# Instruction to the LLM to extract the required fields
INSTRUCTION_TO_LLM = """
Extract the following fields from each tender opportunity. Some fields are visible on the card, 
but most require opening each tender to view details:
1. Title of the project ("Ø§Ø³Ù… Ø§Ù„Ù…Ù†Ø§ÙØ³Ø©")
2. Description ("Ø§Ù„ØºØ±Ø¶ Ù…Ù† Ø§Ù„Ù…Ù†Ø§ÙØ³Ø©")
3. Reference Number ("Ø§Ù„Ø±Ù‚Ù… Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠ")
4. Contract Duration ("Ù…Ø¯Ø© Ø§Ù„Ø¹Ù‚Ø¯")
5. Governmental Entity ("Ø§Ù„Ø¬Ù‡Ø© Ø§Ù„Ø­ÙƒÙˆÙ…ÙŠØ©")
6. Questions Deadline ("Ø¢Ø®Ø± Ù…ÙˆØ¹Ø¯ Ù„Ø¥Ø³ØªÙ„Ø§Ù… Ø§Ù„Ø¥Ø³ØªÙØ³Ø§Ø±Ø§Øª")
7. Bid Deadline ("Ø¢Ø®Ø± Ù…ÙˆØ¹Ø¯ Ù„ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„Ø¹Ø±ÙˆØ¶")
8. Location ("Ù…ÙƒØ§Ù† Ø§Ù„ØªÙ†ÙÙŠØ°")
9. Date of Publication ("ØªØ§Ø±ÙŠØ® Ø§Ù„Ù†Ø´Ø±")
10. Categories/Tags ("Ù†Ø´Ø§Ø· Ø§Ù„Ù…Ù†Ø§ÙØ³Ø©")
11. Bid Price ("Ù‚ÙŠÙ…Ø© ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù…Ù†Ø§ÙØ³Ø©")
"""

# Define a Pydantic model for the extracted data
class TenderOpportunity(BaseModel):
    title: str = Field(..., alias="Ø§Ø³Ù… Ø§Ù„Ù…Ù†Ø§ÙØ³Ø©")
    description: str = Field(..., alias="Ø§Ù„ØºØ±Ø¶ Ù…Ù† Ø§Ù„Ù…Ù†Ø§ÙØ³Ø©")
    reference_number: str = Field(..., alias='Ø§Ù„Ø±Ù‚Ù… Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠ')
    contract_duration: str = Field(..., alias="Ù…Ø¯Ø© Ø§Ù„Ø¹Ù‚Ø¯")
    governmental_entity: str = Field(..., alias="Ø§Ù„Ø¬Ù‡Ø© Ø§Ù„Ø­ÙƒÙˆÙ…ÙŠØ©")
    questions_deadline: str = Field(..., alias="Ø¢Ø®Ø± Ù…ÙˆØ¹Ø¯ Ù„Ø¥Ø³ØªÙ„Ø§Ù… Ø§Ù„Ø¥Ø³ØªÙØ³Ø§Ø±Ø§Øª")
    bid_deadline: str = Field(..., alias="Ø¢Ø®Ø± Ù…ÙˆØ¹Ø¯ Ù„ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„Ø¹Ø±ÙˆØ¶")
    location: str = Field(..., alias="Ù…ÙƒØ§Ù† Ø§Ù„ØªÙ†ÙÙŠØ°")
    date_of_publication: str = Field(..., alias="ØªØ§Ø±ÙŠØ® Ø§Ù„Ù†Ø´Ø±")
    categories_tags: str = Field(..., alias="Ù†Ø´Ø§Ø· Ø§Ù„Ù…Ù†Ø§ÙØ³Ø©")
    bid_price: str = Field(..., alias="Ù‚ÙŠÙ…Ø© ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù…Ù†Ø§ÙØ³Ø©")

async def scrape_page(crawler: AsyncWebCrawler, crawl_config: CrawlerRunConfig, page: int):
    """Scrape a single page of tenders"""
    url = BASE_URL.format(page=page)
    print(f"ğŸ”„ Scraping page {page}...")
    result = await crawler.arun(url=url, config=crawl_config)
    
    if result.success:
        data = json.loads(result.extracted_content)
        print(f"âœ… Found {len(data)} tenders on page {page}")
        return data
    else:
        print(f"âŒ Error scraping page {page}: {result.error_message}")
        return []

async def main():
    # Configure LLM
    llm_config = LLMConfig(
        provider="deepseek/deepseek-chat",
        api_token=os.getenv("DEEPSEEK_API"),
    )

    # Configure LLM extraction strategy with higher limits
    llm_strategy = LLMExtractionStrategy(
        llm_config=llm_config,
        schema=TenderOpportunity.model_json_schema(),
        extraction_type="schema",
        instruction=INSTRUCTION_TO_LLM,
        chunk_token_threshold=2000,  # Increased from 1000
        overlap_rate=0.1,  # Small overlap helps with continuity
        apply_chunking=True,
        input_format="markdown",
        extra_args={
            "temperature": 0.0,
            "max_tokens": 2000,  # Increased from 800
        },
    )

    # Configure the crawler with improved settings
    crawl_config = CrawlerRunConfig(
        extraction_strategy=llm_strategy,
        cache_mode=CacheMode.BYPASS,
        process_iframes=True,  # Changed to True for better JS support
        remove_overlay_elements=True,
        exclude_external_links=True,
        wait_for_page_idle_sec=10,  # Wait for dynamic content
        scroll_to_load_content=True,  # Scroll to load all items
    )

    # Configure the browser
    browser_cfg = BrowserConfig(
        headless=True,
        verbose=True,
        viewport_width=1280,  # Larger viewport helps with responsive sites
        viewport_height=2000,  # Taller viewport to show more items
    )

    all_tenders = []
    
    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        # Scrape multiple pages (adjust range as needed)
        for page in range(1, 6):  # Pages 1 through 5
            page_data = await scrape_page(crawler, crawl_config, page)
            all_tenders.extend(page_data)
            
            # Small delay between pages to avoid rate limiting
            await asyncio.sleep(5)

    # Save all collected data to CSV
    if all_tenders:
        csv_file = "tender_opportunities.csv"
        with open(csv_file, mode='w', encoding='utf-8', newline='') as file:
            writer = csv.DictWriter(file, fieldnames=all_tenders[0].keys())
            writer.writeheader()
            writer.writerows(all_tenders)
        
        print(f"\nğŸ‰ Success! Saved {len(all_tenders)} tenders to {csv_file}")
    else:
        print("âŒ No tender data was collected")

if __name__ == "__main__":
    asyncio.run(main())